---
title: "LASSO cross validation"
output:
  pdf_document: default
  html_notebook: default
---

En esta etapa es importante empezar mostrando cuales de las variables se incluyen en el análisis para tener claridad. En estas simulaciones incluyo las variables "CD4_S24", "CD4porcentajeS24", "CocCD4_CD8_S0" y "CD8porcentajeS24" siguiendo la sugerencia de Enrique. Estas variables tienen coeficientes positivos usando lasso. En el siguiente output se muestran dichas variables.

```{r message=FALSE}
library(glmnet)
library(caret)
library(dplyr)
library(reshape2)

## preprocessing raw data
data <- read.csv("~/scripts/vihCohort/data/TablaLPS_IL18_20180217.csv")


## drop categorical data
data <- data[,sapply(data, is.numeric)]
## list of removed variables
drop <- c("Expediente",
          "IRIS",
          "DeltaCD4_W52",
          "Numero_consecutivo",
          "CD8porcentajeS24",
          "DeltaCD4W52atleast150",
          "CD4_S52",
          "CD4TCabove200_W052",
          "CD4TCabove350_W052",
          "Hepatitis_basal_activa")
input <- data[,!(colnames(data)%in%drop)]

## input definition
input <- as.matrix(input)

## output definition
output <- data$DeltaCD4_W52

## variables included in the analysis
colnames(input)
```

Para tratar de clarificar el análisis y pensando en figuras para el paper. Incluyo la siguiente figura que resume la implementación del análisis. Como se muestra en la imagen y adelantándome a los resultados que se muestran a continuación al parecer será necesario un modelo que incluya la mayor cantidad de observaciones posibles para el training set. Por el momento, estaríamos entonces en la etapa de definición del modelo. 

![**Statistical learning analysis pipeline**](../figures/VIHCohortAnalysis.jpg)



## Implementación de LASSO

Como análisis inicial empezamos implementando LASSO usando la totalidad de observaciones como se muestra en el siguiente código. Se usaron como parámetros nfold = 5 y lambda = lambda.1se. En la figura se muestra la evaluación del performance del modelo ajustado (izquierda). Se puede ver que las predicciones del modelo se correlacionan bien con los datos observados, Lo cual es de esperarse (derecha).

```{r}
## lasso Lambda optimization

set.seed(33)

## model parameters
nfold.coeff <- 5

## performing lasso
lasso <- cv.glmnet(x = input,
                   y = output,
                   nfolds = nfold.coeff)

## model fit performance evaluation
y_predicted <- predict(lasso,
                       newx = input,
                       s = lasso$lambda.1se,
                       type="response")


## plotting validation
par(mfrow=c(1,2))
plot(lasso)
plot(output,y_predicted,main = "Model fit")
abline(0,1,col="red")
```

Para evaluar si el modelo presentaba overfitting evalué el performance (usando MSE como métrica) del ajuste del modelo en el training *vs* test set. En el siguiente código se implementan 100 iteraciones de distintos tamaños del training set (0.6, 0.65, 0.7, ..., 0.9).  

```{r, message=FALSE}
## lasso cross validation

## vector to store coeficients and MSE for trainning and test
res <- matrix(0,0, length(coef(lasso)))
colnames(res) <- rownames(coef(lasso))
MSE.tr <- numeric(0)
MSE.te <- numeric(0)
ratio <- numeric(0)

## lasso iterations
for (p in seq(from=0.6,to = 0.90,by = 0.05)) {
        for (i in 1:100) {
                ## create partition
                inTrain <- createDataPartition(output, p = p, list = FALSE)
                Train   <- input[ inTrain, ] 
                Test <- input[-inTrain,]
                ## perform lasso
                lasso   <- cv.glmnet(x=Train, y=output[inTrain])
                ## obtain coefficients
                lasso.coef <- coef(lasso)
                lasso.coef <- as.vector(lasso.coef)
                res <- rbind(res,lasso.coef)
                ## obtain mse
                predicted.tr <- predict(lasso, 
                                        newx = input[inTrain,], 
                                        s=lasso$lambda.1se)
                predicted.te <- predict(lasso, 
                                        newx = input[-inTrain,], 
                                        s=lasso$lambda.1se)
                MSE.tr <- c(MSE.tr,mean((predicted.tr-output[inTrain])^2))
                MSE.te <- c(MSE.te,mean((predicted.te-output[-inTrain])^2))
                ratio <- c(ratio,p)
        }
}

## transform result matrix to dataframe and add mse column values
res.df <- as.data.frame(res)
res.df <- mutate(res.df, MSE.tr=MSE.tr)
res.df <- mutate(res.df, MSE.te=MSE.te)
res.df <- mutate(res.df, ratio=as.factor(ratio))

mse_train <- aggregate(MSE.tr~as.factor(ratio),res.df,mean)
mse_test <- aggregate(MSE.te~as.factor(ratio),res.df,mean)
#sd_train <- aggregate(MSE.tr~as.factor(ratio),res.df,sd)
#sd_test <- aggregate(MSE.te~as.factor(ratio),res.df,sd)
```

En la siguiente figura se muestra el performance del ajuste de los modelos al variar el tamaño del training. Noten que se muestran dos escalas set en el mismo plot, la escala izquierda (derecha) corresponde al test (training), esto se muestra así dado que el performance del modelo en el training *vs* test set difiere en varios ordenes de magnitud, siendo mejor el performance en el training set. Se puede ver que el ajuste se vuelve mejor conforme aumenta el tamaño del training set. Por lo tanto, esto sugiere que se debe utilizar como modelo LASSO uno que incluya la mayor cantidad posible de muestras. Otra cosa importante es que la variabilidad del modelo es enorme para tamaños de training pequeños. Lo cual de nuevo nos dice que debemos usar un tamaño de training cercano al total de observaciones.

```{r, echo=FALSE}
plot(as.numeric(paste(mse_test[,1])),mse_test[,2],
     col="green", pch=20, lwd=3, type = "l",ylim = c(-100,27000),
     xlab = "test/train ratio", ylab = "MSE", main = "MSE vs train size")
points(as.numeric(paste(mse_test[,1])),mse_test[,2],
     col="green", pch=20,cex=1.5)
#arrows(as.numeric(paste(mse_test[,1])),mse_test[,2]-sd_test[,2],
#         as.numeric(paste(mse_test[,1])),mse_test[,2]+sd_test[,2],
#       code = 3,angle = 90,length = 0.05,lwd=1.5)
par(new=TRUE)
plot(as.numeric(paste(mse_train[,1])),mse_train[,2],
               col="red",pch=20,lwd=3,type="l",
     axes = FALSE, xlab = "", ylab = "")
points(as.numeric(paste(mse_train[,1])),mse_train[,2],
       col="red", pch=20,cex=1.5)
#arrows(as.numeric(paste(mse_train[,1])),mse_train[,2]+sd_train[,2],
#       as.numeric(paste(mse_train[,1])),mse_train[,2]-sd_train[,2],
#       code = 3,angle = 90,length = 0.05,lwd=1.5)
legend("topright",
       legend = c("train","test"),
       fill=c("red","green"),
       inset = 0.05)
axis(side=4)
```

## Conclusiones

Se evaluó el ajuste del modelo de LASSO en el total de observaciones y se observó que existe buena correlación entre los valores predichos y los observados (como era de esperarse). Se evaluó la variabilidad del ajuste al variar el tamaño del training set y se observó que el performance del modelo se incrementa tanto en el training como en el test set conforme aumenta el tamaño del training set. Lo cual sugiere que es necesario usar la mayor cantidad de observaciones posibles. La elección de un modelo así tiene también la ventaja de que presenta menor variabilidad y disminuye el sesgo. 

El siguiente paso sería evaluar la variabilidad del modelo que incluye todas las observaciones (creo que se podría implementar bootstrap) o un modelo que incluya el total - 1 observaciones (usando la estrategia LOOC) (véase el esquema de la primer figura). Creo que una gráfica de iteraciones vs MSE podría darnos una idea de la variabilidad del modelo. También sería necesario una tabla con los promedios de los coeficientes de las variables de LASSO así como sus respectivas desviaciones estandard para ver que tan variables son.  



